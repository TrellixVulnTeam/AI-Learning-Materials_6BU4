{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic 回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上一节课我们学习了简单的线性回归模型，这一次课中，我们会学习第二个模型，Logistic 回归模型。\n",
    "\n",
    "Logistic 回归是一种广义的回归模型，其与多元线性回归有着很多相似之处，模型的形式基本相同，虽然也被称为回归，但是其更多的情况使用在分类问题上，同时又以二分类更为常用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型形式\n",
    "Logistic 回归的模型形式和线性回归一样，都是 y = wx + b，其中 x 可以是一个多维的特征，唯一不同的地方在于 Logistic 回归会对 y 作用一个 logistic 函数，将其变为一种概率的结果。 Logistic 函数作为 Logistic 回归的核心，我们下面讲一讲 Logistic 函数，也被称为 Sigmoid 函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid 函数\n",
    "Sigmoid 函数非常简单，其公式如下\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Sigmoid 函数的图像如下\n",
    "\n",
    "![](https://ws2.sinaimg.cn/large/006tKfTcly1fmd3dde091g30du060mx0.gif)\n",
    "\n",
    "可以看到 Sigmoid 函数的范围是在 0 ~ 1 之间，所以任何一个值经过了 Sigmoid 函数的作用，都会变成 0 ~ 1 之间的一个值，这个值可以形象地理解为一个概率，比如对于二分类问题，这个值越小就表示属于第一类，这个值越大就表示属于第二类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外一个 Logistic 回归的前提是确保你的数据具有非常良好的线性可分性，也就是说，你的数据集能够在一定的维度上被分为两个部分，比如\n",
    "\n",
    "![](https://ws1.sinaimg.cn/large/006tKfTcly1fmd3gwdueoj30aw0aewex.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，上面红色的点和蓝色的点能够几乎被一个绿色的平面分割开来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回归问题 vs 分类问题\n",
    "Logistic 回归处理的是一个分类问题，而上一个模型是回归模型，那么回归问题和分类问题的区别在哪里呢？\n",
    "\n",
    "从上面的图可以看出，分类问题希望把数据集分到某一类，比如一个 3 分类问题，那么对于任何一个数据点，我们都希望找到其到底属于哪一类，最终的结果只有三种情况，{0, 1, 2}，所以这是一个离散的问题。\n",
    "\n",
    "而回归问题是一个连续的问题，比如曲线的拟合，我们可以拟合任意的函数结果，这个结果是一个连续的值。\n",
    "\n",
    "分类问题和回归问题是机器学习和深度学习的第一步，拿到任何一个问题，我们都需要先确定其到底是分类还是回归，然后再进行算法设计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "前一节对于回归问题，我们有一个 loss 去衡量误差，那么对于分类问题，我们如何去衡量这个误差，并设计 loss 函数呢？\n",
    "\n",
    "Logistic 回归使用了 Sigmoid 函数将结果变到 0 ~ 1 之间，对于任意输入一个数据，经过 Sigmoid 之后的结果我们记为 $\\hat{y}$，表示这个数据点属于第二类的概率，那么其属于第一类的概率就是 $1-\\hat{y}$。如果这个数据点属于第二类，我们希望 $\\hat{y}$ 越大越好，也就是越靠近 1 越好，如果这个数据属于第一类，那么我们希望 $1-\\hat{y}$ 越大越好，也就是 $\\hat{y}$ 越小越好，越靠近 0 越好，所以我们可以这样设计我们的 loss 函数\n",
    "\n",
    "$$\n",
    "loss = -(y * log(\\hat{y}) + (1 - y) * log(1 - \\hat{y}))\n",
    "$$\n",
    "\n",
    "其中 y 表示真实的 label，只能取 {0, 1} 这两个值，因为 $\\hat{y}$ 表示经过 Logistic 回归预测之后的结果，是一个 0 ~ 1 之间的小数。如果 y 是 0，表示该数据属于第一类，我们希望 $\\hat{y}$ 越小越好，上面的 loss 函数变为\n",
    "\n",
    "$$\n",
    "loss = - (log(1 - \\hat{y}))\n",
    "$$\n",
    "\n",
    "在训练模型的时候我们希望最小化 loss 函数，根据 log 函数的单调性，也就是最小化 $\\hat{y}$，与我们的要求是一致的。\n",
    "\n",
    "而如果 y 是 1，表示该数据属于第二类，我们希望 $\\hat{y}$ 越大越好，同时上面的 loss 函数变为\n",
    "\n",
    "$$\n",
    "loss = -(log(\\hat{y}))\n",
    "$$\n",
    "\n",
    "我们希望最小化 loss 函数也就是最大化 $\\hat{y}$，这也与我们的要求一致。\n",
    "\n",
    "所以通过上面的论述，说明了这么构建 loss 函数是合理的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们通过例子来具体学习 Logistic 回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a9a7ae9f50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设定随机种子\n",
    "torch.manual_seed(2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们从 data.txt 读入数据，感兴趣的同学可以打开 data.txt 文件进行查看\n",
    "\n",
    "读入数据点之后我们根据不同的 label 将数据点分为了红色和蓝色，并且画图展示出来了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a9a9097438>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3X+MXeV95/H317+wZusEsKdSwth3\nJisnxRC0xGOaRsomG5bFoVm7aaIKY9igprFIQ1Jp0yhEEBQZjRJVq0ZBy1ZySRrwTEA0f2xpRWPU\nAIoahawH8dMmEOOCPVApkzHNhgQvP/LdP84duHPn/jj3zvnxPOd8XtKV771zfO/3nnvu9zzn+zzn\nOebuiIhItawqOwAREcmekruISAUpuYuIVJCSu4hIBSm5i4hUkJK7iEgFKbmLiFSQkruISAUpuYuI\nVNCast5406ZNPj4+Xtbbi4hE6aGHHvq5u4/2W6605D4+Ps7s7GxZby8iEiUzey7NcirLiIhUkJK7\niEgFKbmLiFSQkruISAUpuYuIVFDf5G5m3zKzn5nZE13+bmZ2s5kdM7PHzOw92YeZoZkZGB+HVauS\nf2dmyo5IRCRzaVru3wZ29vj7h4Gtzds+4K9WHlZOZmZg3z547jlwT/7dt08JXkQqp29yd/cfAKd6\nLLIbuN0TDwJnmtnbsgowU9dfD7/+9dLnfv3r5HkRkQrJouZ+DnCy5fFc87llzGyfmc2a2ez8/HwG\nbz2gEycGe15EJFJZJHfr8FzHq267+wF3n3T3ydHRvmfPZm/LlsGez4jK/PWk713KlEVynwM2tzwe\nA17I4HWzNzUFIyNLnxsZSZ7Picr89aTvXcqWRXK/G/hvzVEz7wV+4e7/msHrZm/vXjhwABoNMEv+\nPXAgeT4nKvOXp8yWs753KVuaoZB3AD8C3mVmc2b2STO7xsyuaS5yD3AcOAb8NfCnuUWbhb174dln\n4Te/Sf7NMbGDyvxFak3mmzbBH/9xeS3nGL53lY0qzt1LuW3fvt3roNFwT9LL0lujUXZk1TI97T4y\n0nldl7Heh/3ep6eTZcySf6en84mv0/oaGcnv/SQ7wKynyLE6QzVnJZT5a6lTGaSTolrOw3zvRdbp\nVTaqvuom90COOUso89dS2qSd88CoNwzzvReZcGMoG8nKWNLKL97k5KTndrGOxSZQ6y9lZERZtcLG\nx5OWbi+hbwKrViUt9nZmSRdRlrqtr0Yj6YqScJnZQ+4+2W+5arbcdcxZO53KIGvXwsaN8RwxFXka\nhsqF1VfN5K5jztrpVAb5m7+Bn/+8sIFRK1ZkwlW5sPqqWZbRMadEamYmOcA8cSJpsU9NKeHKUvUu\ny+iYUyJV8GkYUmHVTO465hSRmqtmcgc1gSRIgYzQlRpYU3YAInXRPkJ38SQlUNtDslfdlrtIYDRC\nV4qk5C5SkJhG6Kp8FD8ld5GClHStmIFpLvpqUHIXKUgsI3RVPqoGJXeRgsQyQrdbmajf3D0SFiV3\nkQLFMEK3W5nITKWZmCi5i0Qu687PqakkkbdzV2kmJkruIhHLo/Nz797OUw9DmCN7pDMld5EILbbW\nr7wyn87PRqPz86GN7JHuUiV3M9tpZk+Z2TEzu67D3xtm9n0ze8zMHjCzsexDlRhofHT+Wlvr3ay0\nhR3LyB7prm9yN7PVwC3Ah4FtwB4z29a22P8Abnf3C4D9wFezDlTCp/HRxUhzvdiVtrBjGdkj3aVp\nuV8EHHP34+7+CnAnsLttmW3A95v37+/wdylYGS1ojY8uRr9WeVYt7BhG9vRT5yPJNMn9HOBky+O5\n5nOtHgU+1rz/UWCDmW1ceXgyjLJa0DGdXh+zXq1ytbDfVPcjyTTJvcOgKNr70v8c+ICZPQx8AHge\neG3ZC5ntM7NZM5udn58fOFhJp6wWdCyn18euWz18ejreFnYe6n4kmSa5zwGbWx6PAS+0LuDuL7j7\nH7r7hcD1zed+0f5C7n7A3SfdfXJ0dHQFYUsvZbWg1QlXjF718DqXIdo/e7cO59ocSbp7zxvJnO/H\ngQlgHUkJ5ry2ZTYBq5r3p4D9/V53+/btLonpafdGw90s+Xd6emWv12i4JweiS2+Nxspj7SfrzyLp\nTU+7j4ws/c5HRurxHXT67Gbl/Q7yBMx6n/zqyUdNsRBcBjwNPANc33xuP7Cref/jwE+by9wKnNHv\nNZXcE3n8IOv8I6+zMnfqZev22dsTfJ6/g6IaNpkm9zxuSu6JvH6QakHXT7eWqlnZkeWv22df/C3l\n/TsoskGVNrlbsmzxJicnfXZ2tpT3DsmqVZ1P9TZLhqBJdzMzSefYiRNJp+3UVL07E7vVmRuNpKO1\nysr+7EW+v5k95O6T/ZbT9AMl0wiT4dR9mFurxY7E555bPuFXXTq0y+7MD3EYsJJ7ycreKGMV4jC3\nMkaqtE9F4P5mgq/TmPeyz6gNspGWpnaTx0019zepPj640OrLZXVi17kTNSSqubdQzV1Wouwaa7si\n42nta+j281WfTfGK6gNSzV0qLbRyVlE11/a+hm7UZ1O80ObiUXKXKJVdY21XVM01zYyQ6rMRUHKX\niIXUUirqSKLXkUAIOzkJx5qyAxCpgsVkmnfNdcuWsPoaJFxquYtkpIgjidD6GiRcSu4iEQmtr0HC\npbKMSGT27lUyl/7UchcRqSAldxGRClJyFxGpICV3qbU6X5ZOqk0dqlJbi6fyL57xuThtMKjDUuKn\nlrvUVojTBotkRcldaivECyyIZEXJXWoryAssiGQkVXI3s51m9pSZHTOz6zr8fYuZ3W9mD5vZY2Z2\nWfahimRLp/JLlfVN7ma2GrgF+DCwDdhjZtvaFrsBuMvdLwQuB/5X1oFWloZrlEan8kuVpWm5XwQc\nc/fj7v4KcCewu20ZB97SvP9W4IXsQqwwXeW5dCFMG6z9u+QhTXI/BzjZ8niu+VyrrwBXmtkccA/w\n2UyiqzoN16g97d8lL2mSu3V4rv0CX3uAb7v7GHAZcNDMlr22me0zs1kzm52fnx882qyV3WTScI3S\nlb0JaP8ueUmT3OeAzS2Px1hedvkkcBeAu/8IWA9san8hdz/g7pPuPjk6OjpcxFkJocmk4RqlCmET\n0P5d8pImuR8GtprZhJmtI+kwvbttmRPAxQBmdi5Jcg+gad5DCE0mDdcoVQibgPbv4Sj7KC5rfZO7\nu78GXAscAp4kGRVzxMz2m9mu5mKfBz5lZo8CdwBXu/e6NnsAQmgyabhGqULYBLR/D0MIR3GZc/dS\nbtu3b/dSNRruyfe49NZolBuXFCaUTWB6OnlPs+Tf6eli339YscbdSSjbQhrArKfIsfU9QzXPJlPV\nju8qKpRWcwjDMQdVtZZuCEdxmUuzB8jjVnrL3T2fpsf0tPvIyNLd/8hINM2aEFtjecYU4ueNQUwt\n3TRi+jykbLnXO7nnIaatpE2I+6UQY6qaYXZwZp03c7O8o81HTNuZknsveTbXIt7qQ9wvhRhTlQyb\n1Kr4vcRyFJc2uVuybPEmJyd9dna2+Dduv0IDJIXWrEapjI8nBch2jUZSUA3YqlXJT7SdWVIPLkOI\nMVXJsJtr3j8j6c7MHnL3yX7L1a9DNe/BzaH00g0hxDHXIcZUJcN2JGoUb/jql9zz7haPeKsPcb8U\nYkxV0m0nuWpV/8FeMY7yqZU0tZs8bqXV3KtYLMxQiHXHEGOqik419/ZbqB2LdYVq7l2oWCiyxMxM\nUpU8cSJprb/++vJlIugyqg3V3LtJUzbRSUjR0Fe1cq3llW6d1FGfzFNT9Wu596OWfTQ6fVXr1sGG\nDXDqVFJPnprS1zaIiAd71YZa7sMKYapASaXTV/XKK7CwUI1T4sugDux8FXmkqeTerpKTTFRTmq9E\n++XBRDzYK3hFz8ejskw7HZdGo9tX1U4nPEkIskotKssMK+vjUvX45abTV9WJTniSEBRdFFByb5fl\ncWnV5kUNTPtXtXEjrF27dBnViyUURZ9trbJMnlTiKVzrmG2NlpGQZDUQT2WZEKhztnA6JX45VQbD\nUHRntZJ7njTrVemqltgG/TxVqwzG/n0W2vhIM0cBsBN4CjgGXNfh718HHmnengb+rd9rVvZiHa1i\nugJACrHN8VKx1T/U56nSVEpV+z6HRVYX6wBWA88A7wDWAY8C23os/1ngW/1etxbJ3T2+jNhFjD+s\nKiU29+E+T8TXjlmmat/nsNIm9zRlmYuAY+5+3N1fAe4EdvdYfg9wx0CHD1VWkSJwjCfuVq3LY5jP\n060C6B5fWSPk77NXuaisUtKaFMucA5xseTwH/G6nBc2sAUwA9608NAlJyD+sbrZs6TxYKdYuj2E+\nz9TU8hEaixbr7xBHmyPU77N9FEzreoXuf8t7nadpuVuH57qNn7wc+K67d5g0FMxsn5nNmtns/Px8\n2hglADH2DVdtnpRhPk/rCI1OQj/6ahXq99nrqLbUI95+dRvg94BDLY+/BHypy7IPA+9LUw+qTc29\nImKsubtXpsvjDSv5PFWov4f4ffZar3msc7K6WIeZrSEZAXMx8DxwGLjC3Y+0Lfcu4BAw4f1elJqc\nxFQxOkEobjqnLh+91itkv84zO4nJ3V8DriVJ3E8Cd7n7ETPbb2a7WhbdA9yZJrFLnCrSN1xboZY1\nYtdrvZa6ztM07/O4qSwjUrwQyxpV0Gu9Zr3O0TVUK0h1kWDoq5CypC3LpBkKKSHoNd5KWaVQ+iok\nBppbJhYxnkVUUfoqwhD7PDN5U3KPRYxnEVWUvory5TUhWpV2GEruWShii4jxLKKK0ldRvjyOnqo2\ng6aS+0oVtUVEPo6tSi2iyL+KSsjj6Kly5bY0Q2ryuFVmKGSRU9VFOo4t1rNbe4n0q6iMPH52sZzB\ni4ZCFmTVqmQbaGeWnO0jOjNSMpfVJetaxbKd6jJ7RVEBti91QErW8rhkXdXKbUruK1W1LSIH2v9J\nHrKeDqPoa5zmTcl9paq2RfQyZK+o9n8SiyrNn6QzVLOwd2/cW0EaKzgtc/HPOl1fpDjqUJV0Yult\nEqk4dahKttQrKhIVJXdJR72iIlFRcpd01CsqEhUld0mnTqOCuqjSFApSnLK2GyV3SW9xnNjBg8nj\nq66qTZar2qRSUowyt5t6J3c1xQZX0yxXuUmlpBBlbjf1Te41TVLLDLqDq2mW02AhGUaZ202q5G5m\nO83sKTM7ZmbXdVnmj8zsqJkdMbPvZBtmDmqapJYYZgeX09Ya+kGUBgvJMErdbvpNGwmsBp4B3gGs\nAx4FtrUtsxV4GDir+fi3+71u6VP+xjK/Z56GmTc1h7lWY5gSOIYYJTx5bDeknPI3Tcv9IuCYux93\n91eAO4Hdbct8CrjF3V9s7jB+ttKdTu7UFBuuFZ7DkMgYDqI0WEiGUeZ2kya5nwOcbHk813yu1TuB\nd5rZD83sQTPbmVWAudG47eF2cDlsrbHUs6s0qZQUp6ztJk1ytw7PtU9Is4akNPNBYA9wq5mdueyF\nzPaZ2ayZzc7Pzw8aa7bUFBt+B5fx1qqDKJHspUnuc8DmlsdjwAsdlvk7d3/V3f8FeIok2S/h7gfc\nfdLdJ0dHR4eNOTt1b4oFsoPTQZRI9tIk98PAVjObMLN1wOXA3W3L/G/gPwGY2SaSMs3xLAOVnASw\ngwtkHyNSKX2Tu7u/BlwLHAKeBO5y9yNmtt/MdjUXOwQsmNlR4H7gC+6+kFfQUj0B7GOkIKEPe60K\nzecuIoXJ48LWdaP53KtMTR8JXLdNNIZhr1Why+zFZgWXuxMpQq9NNJZhr1WgskxsdLk7CVyvTRS0\n+a6UyjJVpaaPBK7XJqphr8VRco+NzviRwPXaRDXstThK7rFR00cC128T1bDXYii5x0ZNHwmcNtEw\nqENVRCQi6lAVEakxJXcRkQpSchcRqSAld5GMaXYICYGSu+SnhllumGuOi+RByb2u8k68oWS5gncw\nmhhLQqGhkHVUxLyrIcyBU8L8sqtWJfuydmbJSTsiK5V2KKSSex0VkXhDyHIl7GBC2KdJtWmcu3RX\nxORjIcyBU8Ika5odQkKh5F5HRSTeELJcCTsYnXovoVByr6MiEm/ZWW5mBl56afnzBexgNDGWhEDJ\nvY56Jd4sR5eUleUWO1IX2q7RvnGjmtFSG6mSu5ntNLOnzOyYmV3X4e9Xm9m8mT3SvP1J9qFSy3HT\nuemUeEMZvrhSncYjAvzWbymxS230HS1jZquBp4FLgDngMLDH3Y+2LHM1MOnu16Z944FHy+iy6fmr\nylCPEEbqiOQky9EyFwHH3P24u78C3AnsXmmAA9PZIfnrNoqkU8IPWQgjdURKlia5nwOcbHk813yu\n3cfM7DEz+66Zbe70Qma2z8xmzWx2fn5+sEh17dD8dUt+ZnGVZkIYqVNTqpyGI01ytw7PtR/z/j0w\n7u4XAP8E3Nbphdz9gLtPuvvk6OjoYJGqNZa/qakkkbdzj+sIqeyROjVVlS6bqkhTc/894Cvufmnz\n8ZcA3P2rXZZfDZxy97f2el3V3APVKbkvPq96tfRQlS6b0GVZcz8MbDWzCTNbB1wO3N32Zm9rebgL\neHKQYFNRa6wYjUbn53WEFKUiyySqnIalb3J399eAa4FDJEn7Lnc/Ymb7zWxXc7HPmdkRM3sU+Bxw\ndS7R5jluWsXChOrVlVF0mUSV08C4eym37du3ezCmp91HRtyT30ByGxlJnq+j6Wn3RsPdLPn3059e\n+riu66Wb9vUVyPppNJZu0ou3RiOf99PPqBjArKfIsUru7sX/CmKiX2xvJa6ffvsUs86btVl5McnK\npU3umvIXdNJLL+ol662k9ZNmfIG+umrSlL+DULGws5mZ7icwqZcsUVIvYppz+tR9MrwqdMEpuYN+\nBZ0sNg27qfuOb1FJDYM0+5Q6DDDLIwlXZrx+mtpNHregau7uKha269YPoZr7UiXV3NVNlN+qD33d\nog5VWZFuvXGgxN6uhIZB3vuUGNo6eSXhMjqiB5E2uatDVTpTb1zwZmaSGvuJE0kVaGoqm5JLLCeD\n5zUOIvRNv7odqlkU2arQW5I39UMEL69z+mKZgDWv7o7KbPppmvd53IYqy2RxLKpx2+nFcGwumcuy\nLJHnJpTnTznkTZ9K1twHLbJ1+oZC7y0RKVlWP5Ei2lGtP/GNG5NbiAk5S9VM7oM0KbptWd06CUPp\nLZE4hNy0W6GsknKR7ag6HZBXM7kPsrV0W3b1arXcq6SMJFuDTJJqtfZZqMhRJ3U6IK9mch/kR9Vr\nKF/Ff5i1UVaSrVMm6SbFui9yNYU+fDFL1Uzu7ulbar22rAofUtdKWUm2TpmkmxTrvsh9b532t9VN\n7mnV4NC59spKsiFkkrIbKCnXfVFh1unnruTuXv4PQPJVVpItO5OU/f7uYezg2tTl567kLtVXdJIL\nZdxdCIk1hB1MTaVN7vGdoSqyqMhpD9unClxYgJdfhoMHs7/kYz8hXKx07174xCdg9erk8erVyeOQ\n5ieoOc0tI5JGSBOOhBBLLBPQVFDauWVSJXcz2wl8A1gN3OruX+uy3MeBvwV2uHvPzN0pub/66qvM\nzc1x+vTpvjHFYP369YyNjbF27dqyQ5GVCulqXSEk1hB2MDWVNrmvSfFCq4FbgEuAOeCwmd3t7kfb\nltsAfA748XAhw9zcHBs2bGB8fBwzG/ZlguDuLCwsMDc3x8TERNnhyEpt2dI5mZVx0ZLFBJ7HlJBp\nhVAakp7S1NwvAo65+3F3fwW4E9jdYbmbgL8Ahm52nz59mo0bN0af2AHMjI0bN1bmKCRzsc3MGdpU\ngXlNCZmWLk0ZvDTJ/RzgZMvjueZzbzCzC4HN7v4PKw2oCol9UZU+S6ZivI5ZHa5ZN4jQdnaDiq1x\nMYQ0yb1Thnqj+Ghmq4CvA5/v+0Jm+8xs1sxm5+fn00cp1RLLhOHtym4thyTmnV2MjYshpEnuc8Dm\nlsdjwAstjzcA5wMPmNmzwHuBu81sWcHf3Q+4+6S7T46Ojg4f9aIS97633XYbW7duZevWrdx2222F\nvW8lqF5bDbHu7IZpXMTY0u83EJ6k0/U4MAGsAx4Fzuux/APAZL/X7XQS09GjR9OP5C/xJIqFhQWf\nmJjwhYUFP3XqlE9MTPipU6c6LjvQZ6qLEE7CkfoadNqKwE7YIquTmNz9NeBa4BDwJHCXux8xs/1m\ntiuPHU4qORzaHz58mAsuuIDTp0/zq1/9ivPOO48nnnhi2XKHDh3ikksu4eyzz+ass87ikksu4Xvf\n+97Q71s7sddrJW6DdgZHWkbsOxQSwN3vAe5pe+7GLst+cOVhpZDDof2OHTvYtWsXN9xwAy+//DJX\nXnkl559//rLlnn/+eTZvfrNSNTY2xvPPPz/0+9ZOCEP5pL6mpjqfJ9CtcRFpGTFVcg9STuOOb7zx\nRnbs2MH69eu5+eabOy7jHU5m0ciYAe3dq2Qu5Ri0cRHSOQ4DiHdumZwO7U+dOsVLL73EL3/5y65j\n1MfGxjh58s3RoXNzc7z97W9f0fuKSIEG6QyOtIwYb3LPaSjWvn37uOmmm9i7dy9f/OIXOy5z6aWX\ncu+99/Liiy/y4osvcu+993LppZeu6H1FJFCRDvuMtywDmR/a33777axZs4YrrriC119/nfe9733c\nd999fOhDH1qy3Nlnn82Xv/xlduzYASSlnLPPPjuzOEQkMBGWEYOaFfLJJ5/k3HPPLSWevFTxMw1l\nZkYdqCIZyGziMJEVa5/FcPGMQFCCF8mJknsPjz/+OFddddWS58444wx+/OOhJ76sp17jhJXcRXKh\n5N7Du9/9bh555JGyw4hfpOOERWIW72gZiYemhxUpnJK75C/SccIiMVNyl/xFOk5YJGaquUsxIhwn\nLBKzqFvuZU6xvHPnTs4880w+8pGPFPemIiIpRZvcy76Yyhe+8AUOHjxYzJuJiAwo2uSexxTLaedz\nB7j44ovZsGHD8G8mIpKjaGvueQydTjufu4hI6KJN7nlNsZxmPncRkdBFW5bJa+h0mvncRURCF21y\nz2vodJr53EWCUOZwMQletGUZyH7odNr53AHe//7385Of/ISXXnqJsbExvvnNb+qCHVIczbQpfaSa\nz93MdgLfAFYDt7r719r+fg3wGeB14CVgn7sf7fWams9dZAXGxzt3OjUayWXjpLLSzufetyxjZquB\nW4APA9uAPWa2rW2x77j7u939PwB/AfzlEDGLSFqaaVP6SFOWuQg45u7HAczsTmA38EbL3N3/b8vy\n/w4o5/JOGdN87hKsvIaLSWWkSe7nACdbHs8Bv9u+kJl9BvjvwDpgeZE6QprPXYI1NbW05g6aaVOW\nSDNaxjo8t6xl7u63uPu/B74I3NDxhcz2mdmsmc3Oz893fLOyrumahyp9FgmMZtqUPtIk9zlgc8vj\nMeCFHsvfCfxBpz+4+wF3n3T3ydHR0WV/X79+PQsLC5VIiu7OwsIC69evLzsUqaq9e5PO09/8JvlX\niV1apCnLHAa2mtkE8DxwOXBF6wJmttXdf9p8+PvATxnC2NgYc3NzdGvVx2b9+vWMjY2VHYaI1FDf\n5O7ur5nZtcAhkqGQ33L3I2a2H5h197uBa83sPwOvAi8CnxgmmLVr1zIxMTHMfxURkRapTmJy93uA\ne9qeu7Hl/p9lHJeIiKxAtNMPiIhId0ruIiIVlGr6gVze2Gwe6HAWRiqbgJ9nGE6eYooV4oo3plhB\n8eYpplhhZfE23H35cMM2pSX3lTCz2TRzK4QgplghrnhjihUUb55iihWKiVdlGRGRClJyFxGpoFiT\n+4GyAxhATLFCXPHGFCso3jzFFCsUEG+UNXcREekt1pa7iIj0EHRyN7OdZvaUmR0zs+s6/P0aM3vc\nzB4xs3/ucBGRwvSLtWW5j5uZm1mpPfsp1u3VZjbfXLePmNmflBFnM5a+69bM/sjMjprZETP7TtEx\ntsXSb91+vWW9Pm1m/1ZGnM1Y+sW6xczuN7OHzewxM7usjDhb4ukXb8PMvt+M9QEzK21yJzP7lpn9\nzMye6PJ3M7Obm5/lMTN7T6YBuHuQN5J5bJ4B3kEyR/yjwLa2Zd7Scn8X8L1QY20utwH4AfAgMBn4\nur0a+J+RbAdbgYeBs5qPfzvkeNuW/yzJfE1BxkpSG/508/424NmQ1y3wt8Anmvc/BBwsMd7/CLwH\neKLL3y8D/pFkWvX3Aj/O8v1Dbrm/cQUod3+FZCrh3a0LeDhXgOoba9NNJJchPF1kcB2kjTcEaWL9\nFHCLu78I4O4/KzjGVoOu2z3AHYVEtlyaWB14S/P+W+k93Xfe0sS7Dfh+8/79Hf5eGHf/AXCqxyK7\ngds98SBwppm9Lav3Dzm5d7oC1DntC5nZZ8zsGZKk+bmCYmvXN1YzuxDY7O7/UGRgXaRat8DHmoeL\n3zWzzR3+XoQ0sb4TeKeZ/dDMHmxe0L0sadctZtYAJoD7CoirkzSxfgW40szmSCYP/GwxoXWUJt5H\ngY81738U2GBmGwuIbRipt5VhhJzcM7sCVAF6xmpmq4CvA58vLKLe0qzbvwfG3f0C4J+A23KPqrM0\nsa4hKc18kKQlfKuZnZlzXN2k2m6bLge+6+6v5xhPL2li3QN8293HSMoIB5vbcxnSxPvnwAfM7GHg\nAyTXoHgt78CGNMi2MrCQk3tmV4AqQL9YNwDnAw+Y2bMk9bW7S+xU7btu3X3B3f9f8+FfA9sLiq1d\nmu1gDvg7d3/V3f8FeIok2ZdhkO32csoryUC6WD8J3AXg7j8C1pPMi1KGNNvtC+7+h+5+IXB987lf\nFBfiQAbNcYMpq7MhRWfEGuA4yWHrYufJeW3LbG25/19JLh4SZKxtyz9AuR2qadbt21rufxR4MOBY\ndwK3Ne9vIjnU3RhqvM3l3gU8S/Nck1BjJenwu7p5/1yS5FNKzCnj3QSsat6fAvaXtX6bMYzTvUP1\n91naofp/Mn3vMj94ihVzGfA0SQ/59c3n9gO7mve/ARwBHiHpPOmaUMuOtW3ZUpN7ynX71ea6fbS5\nbn8n4FgN+EvgKPA4cHnI67b5+CvA18qMM+W63Qb8sLkdPAL8l8Dj/TjJZT6fBm4Fzigx1juAfyW5\nQt0cyVHQNcA1zb8bcEvzszyedU7QGaoiIhUUcs1dRESGpOQuIlJBSu4iIhWk5C4iUkFK7iIiFaTk\nLiJSQUruIiIVpOQuIlJB/x/Tb8QWAAAABElEQVTEVuZ0JHEtGwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a9a77f37b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 从 data.txt 中读入点\n",
    "with open('./data.txt', 'r') as f:\n",
    "    data_list = [i.split('\\n')[0].split(',') for i in f.readlines()]\n",
    "    data = [(float(i[0]), float(i[1]), float(i[2])) for i in data_list]\n",
    "\n",
    "# 标准化\n",
    "x0_max = max([i[0] for i in data])\n",
    "x1_max = max([i[1] for i in data])\n",
    "data = [(i[0]/x0_max, i[1]/x1_max, i[2]) for i in data]\n",
    "\n",
    "x0 = list(filter(lambda x: x[-1] == 0.0, data)) # 选择第一类的点\n",
    "x1 = list(filter(lambda x: x[-1] == 1.0, data)) # 选择第二类的点\n",
    "\n",
    "plot_x0 = [i[0] for i in x0]\n",
    "plot_y0 = [i[1] for i in x0]\n",
    "plot_x1 = [i[0] for i in x1]\n",
    "plot_y1 = [i[1] for i in x1]\n",
    "\n",
    "plt.plot(plot_x0, plot_y0, 'ro', label='x_0')\n",
    "plt.plot(plot_x1, plot_y1, 'bo', label='x_1')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们将数据转换成 NumPy 的类型，接着转换到 Tensor 为之后的训练做准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_data = np.array(data, dtype='float32') # 转换成 numpy array\n",
    "x_data = torch.from_numpy(np_data[:, 0:2]) # 转换成 Tensor, 大小是 [100, 2]\n",
    "y_data = torch.from_numpy(np_data[:, -1]).unsqueeze(1) # 转换成 Tensor，大小是 [100, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们来实现以下 Sigmoid 的函数，Sigmoid 函数的公式为\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义 sigmoid 函数\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画出 Sigmoid 函数，可以看到值越大，经过 Sigmoid 函数之后越靠近 1，值越小，越靠近 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a9a9235c18>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a9a8e16f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 画出 sigmoid 的图像\n",
    "\n",
    "plot_x = np.arange(-10, 10.01, 0.01)\n",
    "plot_y = sigmoid(plot_x)\n",
    "\n",
    "plt.plot(plot_x, plot_y, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = Variable(x_data)\n",
    "y_data = Variable(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 PyTorch 当中，不需要我们自己写 Sigmoid 的函数，PyTorch 已经用底层的 C++ 语言为我们写好了一些常用的函数，不仅方便我们使用，同时速度上比我们自己实现的更快，稳定性更好\n",
    "\n",
    "通过导入 `torch.nn.functional` 来使用，下面就是使用方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 logistic 回归模型\n",
    "w = Variable(torch.randn(2, 1), requires_grad=True) \n",
    "b = Variable(torch.zeros(1), requires_grad=True)\n",
    "\n",
    "def logistic_regression(x):\n",
    "    return F.sigmoid(torch.mm(x, w) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在更新之前，我们可以画出分类的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mul() received an invalid combination of arguments - got (numpy.ndarray), but expected one of:\n * (Tensor other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mnumpy.ndarray\u001b[0m)\n * (float other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mnumpy.ndarray\u001b[0m)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-dea74722a624>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplot_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mplot_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mw0\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mplot_x\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mb0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplot_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'g'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cutting line'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: mul() received an invalid combination of arguments - got (numpy.ndarray), but expected one of:\n * (Tensor other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mnumpy.ndarray\u001b[0m)\n * (float other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mnumpy.ndarray\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "# 画出参数更新之前的结果\n",
    "w0 = w[0].data[0]\n",
    "w1 = w[1].data[0]\n",
    "b0 = b.data[0]\n",
    "\n",
    "plot_x = np.arange(0.2, 1, 0.01)\n",
    "plot_y = (-w0 * plot_x - b0) / w1\n",
    "\n",
    "plt.plot(plot_x, plot_y, 'g', label='cutting line')\n",
    "plt.plot(plot_x0, plot_y0, 'ro', label='x_0')\n",
    "plt.plot(plot_x1, plot_y1, 'bo', label='x_1')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到分类效果基本是混乱的，我们来计算一下 loss，公式如下\n",
    "\n",
    "$$\n",
    "loss = -(y * log(\\hat{y}) + (1 - y) * log(1 - \\hat{y}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算loss\n",
    "def binary_loss(y_pred, y):\n",
    "    logits = (y * y_pred.clamp(1e-12).log() + (1 - y) * (1 - y_pred).clamp(1e-12).log()).mean()\n",
    "    return -logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意到其中使用 `.clamp`，这是[文档](http://pytorch.org/docs/0.3.0/torch.html?highlight=clamp#torch.clamp)的内容，查看一下，并且思考一下这里是否一定要使用这个函数，如果不使用会出现什么样的结果\n",
    "\n",
    "**提示：查看一个 log 函数的图像**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6412)\n"
     ]
    }
   ],
   "source": [
    "y_pred = logistic_regression(x_data)\n",
    "loss = binary_loss(y_pred, y_data)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到 loss 之后，我们还是使用梯度下降法更新参数，这里可以使用自动求导来直接得到参数的导数，感兴趣的同学可以去手动推导一下导数的公式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6407)\n"
     ]
    }
   ],
   "source": [
    "# 自动求导并更新参数\n",
    "loss.backward()\n",
    "w.data = w.data - 0.1 * w.grad.data\n",
    "b.data = b.data - 0.1 * b.grad.data\n",
    "\n",
    "# 算出一次更新之后的loss\n",
    "y_pred = logistic_regression(x_data)\n",
    "loss = binary_loss(y_pred, y_data)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的参数更新方式其实是繁琐的重复操作，如果我们的参数很多，比如有 100 个，那么我们需要写 100 行来更新参数，为了方便，我们可以写成一个函数来更新，其实 PyTorch 已经为我们封装了一个函数来做这件事，这就是 PyTorch 中的优化器 `torch.optim`\n",
    "\n",
    "使用 `torch.optim` 需要另外一个数据类型，就是 `nn.Parameter`，这个本质上和 Variable 是一样的，只不过 `nn.Parameter` 默认是要求梯度的，而 Variable 默认是不求梯度的\n",
    "\n",
    "使用 `torch.optim.SGD` 可以使用梯度下降法来更新参数，PyTorch 中的优化器有更多的优化算法，在本章后面的课程我们会更加详细的介绍\n",
    "\n",
    "将参数 w 和 b 放到 `torch.optim.SGD` 中之后，说明一下学习率的大小，就可以使用 `optimizer.step()` 来更新参数了，比如下面我们将参数传入优化器，学习率设置为 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 torch.optim 更新参数\n",
    "from torch import nn\n",
    "w = nn.Parameter(torch.randn(2, 1))\n",
    "b = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "def logistic_regression(x):\n",
    "    return F.sigmoid(torch.mm(x, w) + b)\n",
    "\n",
    "optimizer = torch.optim.SGD([w, b], lr=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  from ipykernel import kernelapp as app\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 200, Loss: 0.39730, Acc: 0.00000\n",
      "epoch: 400, Loss: 0.32458, Acc: 0.00000\n",
      "epoch: 600, Loss: 0.29065, Acc: 0.00000\n",
      "epoch: 800, Loss: 0.27077, Acc: 0.00000\n",
      "epoch: 1000, Loss: 0.25765, Acc: 0.00000\n",
      "\n",
      "During Time: 0.405 s\n"
     ]
    }
   ],
   "source": [
    "# 进行 1000 次更新\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "for e in range(1000):\n",
    "    # 前向传播\n",
    "    y_pred = logistic_regression(x_data)\n",
    "    loss = binary_loss(y_pred, y_data) # 计算 loss\n",
    "    # 反向传播\n",
    "    optimizer.zero_grad() # 使用优化器将梯度归 0\n",
    "    loss.backward()\n",
    "    optimizer.step() # 使用优化器来更新参数\n",
    "    # 计算正确率\n",
    "    mask = y_pred.ge(0.5).float()\n",
    "    acc = (mask == y_data).sum().data[0] / y_data.shape[0]\n",
    "    if (e + 1) % 200 == 0:\n",
    "        print('epoch: {}, Loss: {:.5f}, Acc: {:.5f}'.format(e+1, loss.data[0], acc))\n",
    "during = time.time() - start\n",
    "print()\n",
    "print('During Time: {:.3f} s'.format(during))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到使用优化器之后更新参数非常简单，只需要在自动求导之前使用**`optimizer.zero_grad()`** 来归 0 梯度，然后使用 **`optimizer.step()`**来更新参数就可以了，非常简便\n",
    "\n",
    "同时经过了 1000 次更新，loss 也降得比较低了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们画出更新之后的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mul() received an invalid combination of arguments - got (numpy.ndarray), but expected one of:\n * (Tensor other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mnumpy.ndarray\u001b[0m)\n * (float other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mnumpy.ndarray\u001b[0m)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-d63a599c11ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplot_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mplot_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mw0\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mplot_x\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mb0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplot_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'g'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cutting line'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: mul() received an invalid combination of arguments - got (numpy.ndarray), but expected one of:\n * (Tensor other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mnumpy.ndarray\u001b[0m)\n * (float other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mnumpy.ndarray\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "# 画出更新之后的结果\n",
    "w0 = w[0].data[0]\n",
    "w1 = w[1].data[0]\n",
    "b0 = b.data[0]\n",
    "\n",
    "plot_x = np.arange(0.2, 1, 0.01)\n",
    "plot_y = (-w0 * plot_x - b0) / w1\n",
    "\n",
    "plt.plot(plot_x, plot_y, 'g', label='cutting line')\n",
    "plt.plot(plot_x0, plot_y0, 'ro', label='x_0')\n",
    "plt.plot(plot_x1, plot_y1, 'bo', label='x_1')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到更新之后模型已经能够基本将这两类点分开了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面我们使用了自己写的 loss，其实 PyTorch 已经为我们写好了一些常见的 loss，比如线性回归里面的 loss 是 `nn.MSE()`，而 Logistic 回归的二分类 loss 在 PyTorch 中是 `nn.BCEWithLogitsLoss()`，关于更多的 loss，可以查看[文档](http://pytorch.org/docs/0.3.0/nn.html#loss-functions)\n",
    "\n",
    "PyTorch 为我们实现的 loss 函数有两个好处，第一是方便我们使用，不需要重复造轮子，第二就是其实现是在底层 C++ 语言上的，所以速度上和稳定性上都要比我们自己实现的要好\n",
    "\n",
    "另外，PyTorch 出于稳定性考虑，将模型的 Sigmoid 操作和最后的 loss 都合在了 `nn.BCEWithLogitsLoss()`，所以我们使用 PyTorch 自带的 loss 就不需要再加上 Sigmoid 操作了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 使用自带的loss\n",
    "criterion = nn.BCEWithLogitsLoss() # 将 sigmoid 和 loss 写在一层，有更快的速度、更好的稳定性\n",
    "\n",
    "w = nn.Parameter(torch.randn(2, 1))\n",
    "b = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "def logistic_reg(x):\n",
    "    return torch.mm(x, w) + b\n",
    "\n",
    "optimizer = torch.optim.SGD([w, b], 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6363)\n"
     ]
    }
   ],
   "source": [
    "y_pred = logistic_reg(x_data)\n",
    "loss = criterion(y_pred, y_data)\n",
    "print(loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 200, Loss: 0.39538, Acc: 0.88000\n",
      "epoch: 400, Loss: 0.32407, Acc: 0.87000\n",
      "epoch: 600, Loss: 0.29039, Acc: 0.87000\n",
      "epoch: 800, Loss: 0.27061, Acc: 0.87000\n",
      "epoch: 1000, Loss: 0.25753, Acc: 0.88000\n",
      "\n",
      "During Time: 0.527 s\n"
     ]
    }
   ],
   "source": [
    "# 同样进行 1000 次更新\n",
    "\n",
    "start = time.time()\n",
    "for e in range(1000):\n",
    "    # 前向传播\n",
    "    y_pred = logistic_reg(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    # 反向传播\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # 计算正确率\n",
    "    mask = y_pred.ge(0.5).float()\n",
    "    acc = (mask == y_data).sum().data[0] / y_data.shape[0]\n",
    "    if (e + 1) % 200 == 0:\n",
    "        print('epoch: {}, Loss: {:.5f}, Acc: {:.5f}'.format(e+1, loss.data[0], acc))\n",
    "\n",
    "during = time.time() - start\n",
    "print()\n",
    "print('During Time: {:.3f} s'.format(during))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，使用了 PyTorch 自带的 loss 之后，速度有了一定的上升，虽然看上去速度的提升并不多，但是这只是一个小网络，对于大网络，使用自带的 loss 不管对于稳定性还是速度而言，都有质的飞跃，同时也避免了重复造轮子的困扰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下一节课我们会介绍 PyTorch 中构建模型的模块 `Sequential` 和 `Module`，使用这个可以帮助我们更方便地构建模型"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
